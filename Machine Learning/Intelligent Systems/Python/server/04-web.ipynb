{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №4 (курс \"Практикум по программированию на языке Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполнил: <font color='red'>Никитин Артем Анатольевич, МФТИ, ФПМИ</font>\n",
    "\n",
    "### Тема: Web-сервер для обучения и использования ML-моделей\n",
    "\n",
    "#### Преподаватели: Роман Ищенко (roman.ischenko@gmail.com) и Мурат Апишев (mel-lain@yandex.ru)\n",
    "\n",
    "**Выдана**: 30.04.2024\n",
    "\n",
    "**Дедлайн**: 13.05.2024\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3.7+)\n",
    "\n",
    "#### Правила:\n",
    "\n",
    "Результаты выполнения задания:\n",
    "\n",
    "- архив со скриптами и файлами Dockerfile, который 1-2 команды позволяет развернуть сервер, решающий поставленные в задании задачи\n",
    "- Jupyter Notebook, где __весь код__ из скриптов дублируется (1 ячейка - 1 скрипт) с комментарием, содержащим информацию о том, из какого файла взят код и что верхнеуровнево этот код делает\n",
    "\n",
    "__Максимальное число баллов за задание - 20__.\n",
    "\n",
    "Готовое задание отправляется на почту преподавателя.\n",
    "\n",
    "Задание выполняется самостоятельно. Если какие-то студенты будут уличены в списывании, все они автоматически получат за эту работу 0 баллов. Если вы нашли в Интернете какой-то специфичный код, который собираетесь заимствовать, обязательно укажите это в задании - наверняка вы не единственный, кто найдёт и использует эту информацию.\n",
    "\n",
    "Удалять фрагменты формулировок заданий запрещается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи:\n",
    "\n",
    "**Серверная часть (14 баллов):**\n",
    "\n",
    "- В данной работе нужно написать многозадачный веб-сервер для обучения и инференса ML моделей. На старте сервер получает на вход (через .env) конфиг, в котором должны быть указаны 3 параметра: путь к директории для сохранения моделей внутри контейнера сервера, число ядер, доступных для обучения и максимальное число моделей, которые могут быть одновременно загружены для инференса.\n",
    "\n",
    "\n",
    "- Сервер должен реализовывать следующие методы:\n",
    "    - `fit(X, y, config)` - обучить модель и сохранить на диск по указанным именем\n",
    "    - `predict(y, config)` - предсказать с помощью обученной и загруженной модели по её имени\n",
    "    - `load(config)` - загрузить обученную модель по её имени в режим инференса\n",
    "    - `unload(config)` - выгрузить загруженную модель по её имени\n",
    "    - `remove(config)` - удалить обученную модель с диска по её имени\n",
    "    - `remove_all()` - удалить все обученные модели с диска\n",
    "\n",
    "\n",
    "- Содержимое конфигов и форматы данных предлагается продумать и реализовать самостоятельно\n",
    "- Сервер должен иметь счётчик активных процессов. Максимальное число активных процессов соответствует числу ядер, переданному в конфиге при старте сервиса. Каждое обучение модели запускается в отдельном процессе и до своего завершения потребляет этот процесс. Один процесс всегда остаётся для сервера, в нём же загружаются и работают на инференс обученные модели\n",
    "- Сервер должен корректно обрабатывать все граничные случаи (запуск обучения без свободных ядер, запуск инфренса свыше лимита, запросы с несуществующими именами моделей, запросы с дублирующимися именами моделей)\n",
    "- В реализации должны поддерживаться не менее трёх дискриминативных моделей (т.е. принимающих на вход объекты и метки при обучении и предсказывающих метки для новых объектов)\n",
    "- Сервер должен быть реализован на FastAPI\n",
    "- Проект разворачивается с помощью выбранной библиотеки управления виртуальными окружениями (pipenv, poetry)\n",
    "- Дополнительным плюсом будет использование технологии контейнеризации Docker\n",
    "\n",
    "**Клиентская часть (6 баллов):**\n",
    "\n",
    "- Клиентская часть должна демонстрировать работу с реализованным сервером с помощью библиотек requests и aiohttp. Она может быть реализована непосредственно в Jupyter Notebook, с описанием ожидаемого действия, или в отдельном(-ых) скрипте(-ах), с дублированием в Jupyter Notebook (тогда работоспособность в ноутбуке не требуется). Далее описываются отдельные функции:\n",
    "- Код вызова последовательного вызова обучения как минимум двух (N) различных моделей с таким набором данных и параметрами, чтобы обучение одной модели длилось не менее 60 секунд.\n",
    "- Код вызова асинхронного вызова обучения как минимум двух различных моделей с демонстрацией, что работа выполняется в два (в N) раза быстрее\n",
    "- Асинхронный вызов нескольких предсказаний\n",
    "- Код демонстрации остальных функций сервера (загрузка, выгрузка, удаление)\n",
    "- Должны обрабатываться ошибки и исключения, возвращаемые сервером\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Запускать согласно `README`, код сервера снизу вставлен чисто из-за пункта в задании (в нем не требовалось, чтобы он запускался). Для проверки работы есть код в конце ноутбука с клиентской частью, а так же в папке `client`!"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading configs from file\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_dir: Path\n",
    "    num_cores: int\n",
    "    max_loaded_models: int\n",
    "\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\".env\",\n",
    "        env_file_encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "\n",
    "settings = Settings()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Core functions: for training, for saving (in another process), for loading/unloading, deleting. I use Lock for calculating the number of available cores through the processes.\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import joblib\n",
    "\n",
    "from multiprocessing import get_context\n",
    "from typing import Dict\n",
    "\n",
    "ctx = get_context(\"fork\")\n",
    "Process = ctx.Process\n",
    "\n",
    "_active_trains = ctx.Value('i', 0)\n",
    "_active_lock = ctx.Lock()\n",
    "\n",
    "_loaded_models: Dict[str, object] = {}\n",
    "_loaded_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def _train_and_save(model_name: str, model_cls, X, y, hyperparams: dict):\n",
    "    try:\n",
    "        model = model_cls(**hyperparams)\n",
    "        model.fit(X, y)\n",
    "        path = os.path.join(settings.model_dir, f\"{model_name}.joblib\")\n",
    "        joblib.dump(model, path)\n",
    "    finally:\n",
    "        with _active_lock:\n",
    "            _active_trains.value -= 1\n",
    "\n",
    "\n",
    "def start_training(model_name: str, model_type: str, X, y, hyperparams: dict):\n",
    "    path = os.path.join(settings.model_dir, f\"{model_name}.joblib\")\n",
    "    if os.path.exists(path):\n",
    "        raise FileExistsError(f\"Model '{model_name}' already exists\")\n",
    "\n",
    "    with _active_lock:\n",
    "        if _active_trains.value >= settings.num_cores:\n",
    "            raise RuntimeError(\"No free cores for training\")\n",
    "        _active_trains.value += 1\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    cls_map = {\n",
    "        \"logreg\": LogisticRegression,\n",
    "        \"rf\": RandomForestClassifier,\n",
    "        \"svm\": SVC,\n",
    "    }\n",
    "    model_cls = cls_map.get(model_type)\n",
    "    if model_cls is None:\n",
    "        with _active_lock:\n",
    "            _active_trains.value -= 1\n",
    "        raise ValueError(f\"Unknown model type '{model_type}'\")\n",
    "\n",
    "    proc = Process(\n",
    "        target=_train_and_save,\n",
    "        args=(model_name, model_cls, X, y, hyperparams),\n",
    "    )\n",
    "    proc.start()\n",
    "    return proc.pid\n",
    "\n",
    "\n",
    "def available_cores() -> int:\n",
    "    with _active_lock:\n",
    "        return settings.num_cores - _active_trains.value\n",
    "\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    with _loaded_lock:\n",
    "        if model_name in _loaded_models:\n",
    "            return\n",
    "        if len(_loaded_models) >= settings.max_loaded_models:\n",
    "            raise RuntimeError(\"Loaded models limit reached\")\n",
    "        path = os.path.join(settings.model_dir, f\"{model_name}.joblib\")\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Model '{model_name}' not found on disk\")\n",
    "        _loaded_models[model_name] = joblib.load(path)\n",
    "\n",
    "\n",
    "def unload_model(model_name: str):\n",
    "    with _loaded_lock:\n",
    "        if model_name not in _loaded_models:\n",
    "            raise KeyError(f\"Model '{model_name}' is not loaded\")\n",
    "        del _loaded_models[model_name]\n",
    "\n",
    "\n",
    "def predict(model_name: str, X):\n",
    "    with _loaded_lock:\n",
    "        if model_name not in _loaded_models:\n",
    "            raise KeyError(f\"Model '{model_name}' is not loaded\")\n",
    "        return _loaded_models[model_name].predict(X).tolist()\n",
    "\n",
    "\n",
    "def remove_model(model_name: str):\n",
    "    with _loaded_lock:\n",
    "        _loaded_models.pop(model_name, None)\n",
    "    path = os.path.join(settings.model_dir, f\"{model_name}.joblib\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Model '{model_name}' not found on disk\")\n",
    "    os.remove(path)\n",
    "\n",
    "\n",
    "def remove_all():\n",
    "    with _loaded_lock:\n",
    "        _loaded_models.clear()\n",
    "    for fname in os.listdir(settings.model_dir):\n",
    "        if fname.endswith(\".joblib\"):\n",
    "            os.remove(os.path.join(settings.model_dir, fname))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Request types and schemas\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    model_name: str\n",
    "    model_type: str\n",
    "    hyperparams: Optional[Dict] = {}\n",
    "\n",
    "\n",
    "class FitRequest(BaseModel):\n",
    "    X: List[List[float]]\n",
    "    y: List[int]\n",
    "    config: ModelConfig\n",
    "\n",
    "\n",
    "class FitResponse(BaseModel):\n",
    "    status: str\n",
    "    pid: int\n",
    "\n",
    "\n",
    "class LoadRequest(BaseModel):\n",
    "    model_name: str\n",
    "\n",
    "\n",
    "class UnloadRequest(LoadRequest): pass\n",
    "\n",
    "\n",
    "class RemoveRequest(LoadRequest): pass\n",
    "\n",
    "\n",
    "class GenericResponse(BaseModel):\n",
    "    status: str\n",
    "\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    model_name: str\n",
    "    X: List[List[float]]\n",
    "\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    predictions: List[int]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# API realization\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.post(\"/fit\", response_model=FitResponse)\n",
    "def fit(req: FitRequest):\n",
    "    try:\n",
    "        pid = start_training(\n",
    "            req.config.model_name,\n",
    "            req.config.model_type,\n",
    "            req.X, req.y, req.config.hyperparams or {}\n",
    "        )\n",
    "    except FileExistsError as e:\n",
    "        raise HTTPException(400, str(e))\n",
    "    except RuntimeError as e:\n",
    "        raise HTTPException(429, str(e))\n",
    "    return FitResponse(status=\"training_started\", pid=pid)\n",
    "\n",
    "\n",
    "@app.post(\"/load\", response_model=GenericResponse)\n",
    "def load(req: LoadRequest):\n",
    "    try:\n",
    "        load_model(req.model_name)\n",
    "    except FileNotFoundError:\n",
    "        raise HTTPException(404, \"Model not found\")\n",
    "    except RuntimeError as e:\n",
    "        raise HTTPException(429, str(e))\n",
    "    return GenericResponse(status=\"loaded\")\n",
    "\n",
    "\n",
    "@app.post(\"/unload\", response_model=GenericResponse)\n",
    "def unload(req: UnloadRequest):\n",
    "    try:\n",
    "        unload_model(req.model_name)\n",
    "    except KeyError:\n",
    "        raise HTTPException(404, \"Model not loaded\")\n",
    "    return GenericResponse(status=\"unloaded\")\n",
    "\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictResponse)\n",
    "def do_predict(req: PredictRequest):\n",
    "    try:\n",
    "        preds = predict(req.model_name, req.X)\n",
    "    except KeyError:\n",
    "        raise HTTPException(404, \"Model not loaded\")\n",
    "    return PredictResponse(predictions=preds)\n",
    "\n",
    "\n",
    "@app.post(\"/remove\", response_model=GenericResponse)\n",
    "def remove(req: RemoveRequest):\n",
    "    try:\n",
    "        remove_model(req.model_name)\n",
    "    except FileNotFoundError:\n",
    "        raise HTTPException(404, \"Model not found\")\n",
    "    return GenericResponse(status=\"removed\")\n",
    "\n",
    "\n",
    "@app.post(\"/remove_all\", response_model=GenericResponse)\n",
    "def removeall():\n",
    "    remove_all()\n",
    "    return GenericResponse(status=\"all_removed\")\n",
    "\n",
    "\n",
    "@app.get(\"/cores\")\n",
    "def get_cores():\n",
    "    return {\n",
    "        \"free_cores\": available_cores(),\n",
    "        \"total_cores\": settings.num_cores\n",
    "    }\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Запускать отсюда после запуска сервера!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:35:40.937092Z",
     "start_time": "2025-05-27T11:35:40.852282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "\n",
    "URL = \"http://localhost:8000\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:42:32.285122Z",
     "start_time": "2025-05-27T11:42:32.276937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resp = requests.post(f\"{URL}/remove_all\")\n",
    "print(\"remove_all:\", resp.json())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_all: {'status': 'all_removed'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:43:06.385755Z",
     "start_time": "2025-05-27T11:42:34.646927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wait_for_model(name, timeout=3600, poll=5):\n",
    "    import os\n",
    "\n",
    "    start = time.time()\n",
    "    path = f\"./app/models/{name}.joblib\"\n",
    "    while time.time() - start < timeout:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Model {name} is ready\")\n",
    "            return True\n",
    "        time.sleep(poll)\n",
    "    raise TimeoutError(f\"Model {name} not ready in {timeout}s\")\n",
    "\n",
    "\n",
    "def train_model(name, model_type):\n",
    "    X = np.random.randn(10000, 50).tolist()\n",
    "    y = np.random.randint(0, 2, size=10000).tolist()\n",
    "    payload = {\"X\": X, \"y\": y, \"config\": {\"model_name\": name, \"model_type\": model_type}}\n",
    "    resp = requests.post(f\"{URL}/fit\", json=payload)\n",
    "    print(name, resp.json())\n",
    "    if resp.status_code == 200:\n",
    "        wait_for_model(name)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "train_model(\"m1\", \"rf\")\n",
    "train_model(\"m2\", \"svm\")\n",
    "train_model(\"m3\", \"logreg\")\n",
    "train_model(\"m4\", \"svm\")\n",
    "train_model(\"m5\", \"svm\")\n",
    "print(\"Sequential total:\", time.time() - start)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 {'status': 'training_started', 'pid': 19479}\n",
      "Current working directory: /home/ntheme/Data1/Workfiles/Programming/Projects/MIPT/Machine Learning/Intelligent Systems/Python/server\n",
      "Model m1 is ready\n",
      "m2 {'status': 'training_started', 'pid': 19517}\n",
      "Current working directory: /home/ntheme/Data1/Workfiles/Programming/Projects/MIPT/Machine Learning/Intelligent Systems/Python/server\n",
      "Model m2 is ready\n",
      "m3 {'status': 'training_started', 'pid': 19534}\n",
      "Current working directory: /home/ntheme/Data1/Workfiles/Programming/Projects/MIPT/Machine Learning/Intelligent Systems/Python/server\n",
      "Model m3 is ready\n",
      "m4 {'status': 'training_started', 'pid': 19577}\n",
      "Current working directory: /home/ntheme/Data1/Workfiles/Programming/Projects/MIPT/Machine Learning/Intelligent Systems/Python/server\n",
      "Model m4 is ready\n",
      "m5 {'status': 'training_started', 'pid': 19589}\n",
      "Current working directory: /home/ntheme/Data1/Workfiles/Programming/Projects/MIPT/Machine Learning/Intelligent Systems/Python/server\n",
      "Model m5 is ready\n",
      "Sequential total: 31.735300540924072\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:43:42.128016Z",
     "start_time": "2025-05-27T11:43:40.543837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def train(session, name, model_type):\n",
    "    X = np.random.randn(10000, 50).tolist()\n",
    "    y = np.random.randint(0, 2, size=10000).tolist()\n",
    "    payload = {\"X\": X, \"y\": y, \"config\": {\"model_name\": name, \"model_type\": model_type}}\n",
    "    async with session.post(f\"{URL}/fit\", json=payload) as resp:\n",
    "        print(name, await resp.json())\n",
    "\n",
    "\n",
    "async def async_training():\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        await asyncio.gather(\n",
    "            train(session, \"a1\", \"rf\"),\n",
    "            train(session, \"a2\", \"svm\"),\n",
    "            train(session, \"a3\", \"logreg\"),\n",
    "            train(session, \"a4\", \"svm\"),\n",
    "            train(session, \"a5\", \"svm\"),\n",
    "        )\n",
    "    print(\"Async total:\", time.time() - start)\n",
    "\n",
    "\n",
    "await async_training()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 {'status': 'training_started', 'pid': 19638}\n",
      "a5 {'status': 'training_started', 'pid': 19639}\n",
      "a4 {'status': 'training_started', 'pid': 19640}\n",
      "a3 {'status': 'training_started', 'pid': 19642}\n",
      "a1 {'status': 'training_started', 'pid': 19670}\n",
      "Async total: 1.581207513809204\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:46:30.701365Z",
     "start_time": "2025-05-27T11:46:30.639601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"load m1:\", requests.post(f\"{URL}/load\", json={\"model_name\": \"m1\"}).json())\n",
    "print(\"predict m1:\", requests.post(f\"{URL}/predict\", json={\"model_name\": \"m1\", \"X\": [[0.1] * 50]}).json())\n",
    "print(\"unload m1:\", requests.post(f\"{URL}/unload\", json={\"model_name\": \"m1\"}).json())\n",
    "print(\"remove m1:\", requests.post(f\"{URL}/remove\", json={\"model_name\": \"m1\"}).json())\n",
    "print(\"remove_all:\", requests.post(f\"{URL}/remove_all\").json())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load m1: {'status': 'loaded'}\n",
      "predict m1: {'predictions': [0]}\n",
      "unload m1: {'status': 'unloaded'}\n",
      "remove m1: {'status': 'removed'}\n",
      "remove_all: {'status': 'all_removed'}\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
