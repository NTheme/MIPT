\section{Метод окна Парзена.}

Напомним идею метрического классификатора. Будем обозначать $x = (x^1, ..., x^n)$ - вектор признаков объекта $x$, $x_i = (x_i^1, ..., x_i^n)$ - вектор признаков объекта $x_i$. Пусть на пространстве признаков задана метрика $\rho$. Для произвольного объекта $x$ отранжируем объекты обучающей выборки $x_1, x_2, ..., x_l$:
\begin{equation*}
    \rho(x, x^{(1)}) \le \rho(x, x^{(2)}) \le ... \le \rho(x, x^{(l)}).
\end{equation*}
Таким образом, $x^{(i)}$ - $i-$й ближайший сосед объекта $x$ среди обучающий выборки, обозначим через $y^{(i)}$ ответ на нём. 

Метрический классификатор предлагает следующую модель зависимости:
\begin{equation*}
    \displaystyle a(x; X^l) = \arg\max_{y\in Y} \sum\limits_{i=1}^l[y^{(i)}=y]w(i,x), 
\end{equation*}
где $w(i,x)$ - некоторый вес, отражающий степень близости к объекту $x$ его $i-$го соседа. Вес неотрицателен и не возрастает по $i$. 

Кроме того, введём обозначение:
\begin{equation*}
    \displaystyle \Gamma_y(x) = \sum\limits_{i=1}^l[y^{(i)}=y]w(i,x) - \text{оценка близости объекта $x$ к классу $y$}. 
\end{equation*}

Напомним также, что метод $k$ ближайших соседей заключается в выборе в качестве весовой функции $w(i,x) = [i\le k]$. Среди недостатков этого метода выделим следующие:
\begin{itemize}
    \item в силу того, что функция близости дискретнозначная (принимает не более $k$ значений), часто попадаем в ситуацию неоднозначности классификации, когда $\Gamma_y(x) = \Gamma_z(x)$, $y\ne z$;
    \item метод не учитвает значение расстояний от объекта до ближайших соседей. Естественным кажется использование меньшего веса для далёкого объекта, даже если он попадаёт в $k$ ближайших.
\end{itemize}

Для борьбы с этими недостатков модифицируем веса следующим образом:
\begin{equation*}
    w(i,x) = [i\le k]w_i, \text{где $w_i$ зависит только от номера соседа}.
\end{equation*}

Тем самым, получим метод $k$ взвешенных ближайших соседей. В качестве $w_i$ можно брать, например, линейно убывающие веса $\displaystyle w_i = \frac{k+1-i}{k}$ или экспоненциально убывающие веса $w_i = q^i$, $0 < q < 1$.

Однако линейно убывающие веса всё ещё допускают неоднозначность классификации. Кроме того, метод по-прежнему не учитывает расстояния между объектами. 

Наконец, положим
\begin{equation*}
    \displaystyle w(i,x) = K\left(\frac{\rho(x, x^{(i)})}{h}\right),
\end{equation*}
где $K(r)$ - невозрастающая функция, определённая на неотрицательных числах, называемая ядром, положительная на отркезке $[0,1]$ и равная нулю вне его, $h$ - ширина окна. 

Таким образом, получим метод окна Парзена фиксированной ширины:
\begin{equation*}
    \displaystyle a(x; X^l, h, K) = \arg\max_{y\in Y} \sum\limits_{i=1}^l[y^{(i)}=y]K\left(\frac{\rho(x, x^{(i)})}{h}\right). 
\end{equation*}

Мы видим, что такой метод для каждого объекта $x$ рассматривает только объекты обучающей выборки, находящиеся на расстоянии не больше $h$ от $x$. Причём, чем дальше объект от $x$, тем  меньший вклад он даёт в оценку близости.

\textit{Замечание.} Вообще говоря, в качестве ядра можно брать функции, которые принимают ненулевые значения вне отрезка $[0,1]$, однако тогда они подбираются быстро убывающими (см. далее гауссовское ядро).

Заметим, что если расстояние от объекта $x$ до всех объектов обучающей выборки больше $h$, то построенный метод не может классифицировать $x$, так как в таком случае $\displaystyle\sum\limits_{i=1}^l[y^{(i)}=y]K\left(\frac{\rho(x, x^{(i)})}{h}\right) \equiv 0$. Это соображение наталкивает на использование метода окна Парзена переменной ширины:

\begin{equation*}
    \displaystyle a(x; X^l, h, K) = \arg\max_{y\in Y} \sum\limits_{i=1}^l[y^{(i)}=y]K\left(\frac{\rho(x, x^{(i)})}{\rho(x, x^{(k+1)})}\right). 
\end{equation*}

\textit{Замечание.} В знаменателе в ядре стоит расстояние до $k+1-$го соседа, так как на практике зачастую берутся ядра, которые в точке $1$ равны $0$. Таким образом, метод окна Парзена переменной ширины учитывает именно $k$ ближайших соседей объекта.

Для подбора наилучшей модели оптимизируются параматры:
\begin{itemize}
    \item ширина окна $h$ или количество сосеседей $k$;
    \item ядро $K$.
\end{itemize}

Выше мы считали, что ядро определено на неотрицательной полуоси. Эквивалентно, можно считать, что ядро определено на всей действительной оси, является чётной функцией, невозрастающей на отрезке $[0,1]$ (и, как правило, равна $0$ вне отрезка $[-1,1]$). В таком случае расстояние между объектами можно понимать как ориентированное. Однако, нетружно видеть, что ориентация ни на что не влияет. Приведём примеры наиболее часто используемых ядер:
\begin{itemize}
    \item $\displaystyle K_1(r) = \frac{3}{4}(1-r^2)[r\le1]$ - ядро Епанечникова;
    \item $\displaystyle K_2(r) = \frac{15}{16}(1-r^2)^2[r\le1]$ - квартическое ядро;
    \item $\displaystyle K_3(r) = (1-|r|)[r\le1]$ - треугольное ядро; 
    \item $\displaystyle K_4(r) = \frac{1}{2}[r\le1]$ - прямоугольное ядро; \item $\displaystyle K_5(r) = \frac{1}{\sqrt{2\pi}}e^{-r^2/2}$ - гауссовское ядро.
\end{itemize}
Числовые коэффициенты выбираются из соображений нормировки: $\displaystyle\int\limits_{-\infty}^{+\infty} K_i(r)dr = 1$.

\textbf{Задача 1.} Докажите, что метод окна Парзена фиксированной ширины можно переписать следующим образом:

\begin{equation*}
    \displaystyle a(x; X^l, h, K) = \arg\max_{y\in Y} \sum\limits_{i=1}^l[y_i=y]K\left(\frac{\rho(x, x_i)}{h}\right),
\end{equation*}
т.е. объекты обучающей выборки не обязательно ранжировать по расстоянию до объекта.

\textbf{Доказательство.} Заметим, что при любом фиксированном $y\in Y$ справедливо равенство $\displaystyle\sum\limits_{i=1}^l[y_i=y]K\left(\frac{\rho(x, x_i)}{h}\right) = \sum\limits_{i=1}^l[y^{(i)}=y]K\left(\frac{\rho(x, x^{(i)})}{h}\right)$, поскольку каждый объект обучающей выборки в обеих суммах участвует лишь единожды и значение соответствующего слагаемого не зависит от номера объекта. Отметим, что метод окна Парзена переменной ширины также можно переписать аналогичным образом.
\\

\textbf{Задача 2.} Вася решает методом окна Парзена фиксированной ширины задачу классификации объектов на $8$ классов по $3$ вещественным признакам. Все признаки объектов в обучающей выборке принимают значения, по модулю не меньшие $1$, и объекты $i-$го класса попадают в $i-$й октант пространства признаков. Вася использует евклидову метрику на пространстве признаков и берёт ширину окна $h = 1$. К какому из классов может его классификатор отнести точку $0$?

\textbf{Ответ.} Ни к какому.

\textbf{Решение.} По условию расстояние между $0$ и объектом обучающей выборки $\rho(x_i,0)\ge\sqrt{(1-0)^2+(1-0)^2+(1-0)^2} = \sqrt{3} > 1 = h$. Таким образом, ни одна точка обучающей выобрки не попадает в нужное окно с центром в нуле.

\textbf{Задача 3.} Докажите, что в качестве ядер в методе окна Парзена фиксированной ширины также можно брать выпуклую комбинацию ядер $K_1$, $K_2$, $K_3$, $K_4$ из примеров выше.

\textbf{Доказательство.} Заметим, что выпуклая комбинация также будет положительна при $|r|\le 1$ и нулевая вне этого отрезка, чётна и не убывает на отрезке $[0,1]$. Более того, также сохраняется нормировка: 
\begin{equation*}
    \displaystyle\int\limits_{-1}^{1}\left[\alpha_1K_1(r)+\alpha_2K_2(r) +\alpha_3K_3(r)+\alpha_4K_4(r)\right]dr = \alpha_1 + \alpha_2 +\alpha_3 +\alpha_4 =1.
\end{equation*}
