\section{Билет 5}

\subsection{Удовлетворитель. Сопряженные направления. Интуиция метода сопряженных градиентов:
    как работает, чего хотим добиться, почему именно так строим метод.
    Характер сходимости для систем линейных уравнение с положительно
    определенной матрицей}

\begin{definition}
    Множество векторов $\{ p_i \}_{i=0}^{n-1}$ будем называть сопряженным относительно положительно определенной матрицы $A$,
    если для любых $i \neq j \in \{0, \dots, n - 1\}$ следует
    $$p_i^T A p_j = 0.$$
\end{definition}

\begin{theorem}
    Сопряженные векторы $\{ p_i \}_{i=0}^{n-1}$ являются линейно независимыми.
\end{theorem}
\begin{proof}
    От противного: пусть существует $p_i$ такое, что существуют:
    $$p_i = \sum_{i \neq j} \lambda_j p_j \quad \text{для некоторых} \quad \lambda_j \in \mathbb{R}.$$

    По определению сопряженных векторов:
    $$0 = p_m^T A p_i = \sum_{i \neq j} \lambda_j p_m^T A p_j = \lambda_m p_m^T A p_m.$$

    Получили $\lambda_m = 0$.
    Из этого следует, что для $m \neq i$ и получаем, что $\lambda_m = 0$, а значит $$p_i = 0.$$
    Противоречие.
\end{proof}

Если у нас есть $d$ сопряженных векторов, то они формируют базис. Раскладываем решение:
$$x^* = \sum_{i=0}^{d-1} \lambda_i p_i.$$

Чтобы найти $\lambda_i$, возьмем скалярное произведение с $A p_j$:
$$p_j^T A x^* = \sum_{i=0}^{d-1} \lambda_i p_j^T A p_i = \lambda_j p_j^T A p_j.$$

Здесь опять воспользовались определением сопряженности. Заметим, что $A x^* = b$, тогда
$$p_j^T b = \lambda_j p_j^T A p_j.$$

Тогда
$$\lambda_j = \frac{p_j^T b}{p_j^T A p_j}.$$

Остается непонятно, как получать сопряженные направления на практике и существуют ли они вообще? Начнем превращать рассуждения в некоторый итеративный метод:
$$x_{k+1} = x_k + \alpha_k p_k.$$

Т.е. предполагается, что мы на каждой итерации будем искать новое $p_k$ и подбивать к нему $\alpha_k$.

Мы уже более менее поняли, как искать $\alpha$, когда искали $\lambda$. Верно ли равенство $\lambda = \alpha$? Не всегда. Итеративная схема с $\alpha$:
$$ x_{k+1} = x_0 + \sum_{i=0}^{k} \alpha_i p_i. $$

Итеративная схема с $\lambda$:
$$ x_{k+1} = \sum_{i=0}^{k} \lambda_i p_i. $$

Получается, что $\alpha_i = \lambda_i$, если $x_0 = 0$. Нужна формула поиска $\alpha$, так как стартовать из 0 хорошо, но, возможно, у нас есть более близкий кандидат в качестве стартовой точки.

Можно разложить $x_0$ по базису и найти для него $\tilde{\lambda}_i$:
$$ x_0 = \sum_{i=0}^{d-1} \tilde{\lambda}_i p_i, \quad \text{где} \quad \tilde{\lambda}_i = \frac{p_i^T A x_0}{p_i^T A p_i}. $$

Тогда справедливо следующее утверждение:
$$ x_0 + \sum_{i=0}^{d-1} \alpha_i p_i = \sum_{i=0}^{d-1} \left( \frac{p_i^T A x_0}{p_i^T A p_i} + \alpha_i \right) p_i = \sum_{i=0}^{d-1} \lambda_i p_i = \sum_{i=0}^{d-1} \frac{p_i^T b}{p_i^T A p_i} p_i. $$

Получаем
$$ \alpha_k = \frac{p_k^T (b - A x_0)}{p_k^T A p_k}. $$

Результат уже нормальный, можно чуть-чуть еще докрутить:
$$ p_k^T A (x_k - x_0) = 0. $$

Это верно, т.к. $(x_k - x_0) = \sum_{i=0}^{k-1} \alpha_i p_i$, а $p_i$ и $p_k$ сопряженные относительно $A$. Тогда можно переписать так:
$$ \alpha_k = \frac{p_k^T (b - A x_k)}{p_k^T A p_k} = -\frac{p_k^T r_k}{p_k^T A p_k}, $$

где введено обозначение $r_k = A x_k - b$.

Рассмотрим шаг метода $x_{k+1} = x_k + \alpha_k p_k$, а также функцию
$$ f(x) = \frac{1}{2} x^T A x - b^T x. $$

Минимизация этой функции эквивалентна поиску решения системы: $\nabla f(x^*) = A x^* - b = 0$.

Рассмотрим:
$$ g(\alpha) = f(x_k + \alpha p_k). $$

Минимум этой функции в точке $\alpha^* = \frac{p_k^T (b - A x_k)}{p_k^T A p_k} = \alpha_k$. Это и есть минимизация вдоль $p_k$.

\begin{theorem}
    Если $\{p_i\}_{i=0}^k$ — сопряженные направления, то для любого $k \geq 0$ и $i \leq k$ справедливо:
    $$ r_{k+1}^T p_i = 0 \Leftrightarrow \langle \nabla f(x_{k+1}), p_i \rangle = 0. $$
\end{theorem}
\begin{proof}
    База: $r_1 = Ax_1 - b = Ax_0 - b + \alpha_0 A p_0 = r_0 + \alpha_0 A p_0$, в силу выбора $\alpha_0 = 0$:
    $$ p_0^T r_1 = p_0^T r_0 + \alpha_0 p_0^T A p_0 = 0. $$

    Предположение: пусть предположение верно для всех $i \leq k$.

    Переход: докажем для $k+1$. Рассмотрим:
    $$ r_{k+1} = Ax_{k+1} - b = Ax_k - b + \alpha_k A p_k = r_k + \alpha_k A p_k. $$
    Откуда в силу выбора $\alpha_k$:
    $$ p_k^T r_{k+1} = p_k^T r_k + \alpha_k p_k^T A p_k = 0. $$

    Для $i < k$:
    $$ p_i^T r_{k+1} = p_i^T r_k + \alpha_k p_i^T A p_k = 0. $$
    Это верно в силу индукции и сопряженности. $\square$

    Перейдем к поиску $p_k$. Для этого хотим потребовать «дешевизну» подсчета $p_k$:
    $$ p_k = -r_k + \beta_k p_{k-1}, $$
    где $\beta_k$ — некоторый коэффициент. Чтобы найти $p_k$, нужно знать только $p_{k-1}$ и $r_k$, а старые $r_i$ и $p_i$ можно уже забыть (они учтены в $x_k$).

    Как найти $\beta_k$? Сопряженность $p_k$ и $p_{k-1}$:
    $$ 0 = p_{k-1}^T A p_k = -p_{k-1}^T A r_k + \beta_k p_{k-1}^T A p_{k-1}, $$
    откуда
    $$ \beta_k = \frac{p_{k-1}^T A r_k}{p_{k-1}^T A p_{k-1}}. $$
\end{proof}

\begin{algorithm}[H]
    \caption{Метод сопряженных градиентов}
    \textbf{Вход:} стартовая точка $x_0 \in \mathbb{R}^d$, $r_0 = A x_0 - b$, $p_0 = -r_0$, количество итераций $K$
    \begin{algorithmic}[1]
        \For{$k = 0, 1, \dots, K - 1$}
        \State $\alpha_k = \frac{-r_k^T p_k}{p_k^T A p_k}$
        \State $x_{k+1} = x_k + \alpha_k p_k$
        \State $r_{k+1} = A x_{k+1} - b$
        \State $\beta_{k+1} = \frac{r_{k+1}^T A p_k}{p_k^T A p_k}$
        \State $p_{k+1} = -r_{k+1} + \beta_{k+1} p_k$
        \EndFor
    \end{algorithmic}
    \textbf{Выход:} $x_K$
\end{algorithm}

\subsection{Хор Турецкого. Формулировка оценки сходимости метода сопряженных градиентов (2 результата):
    в зависимости от \texorpdfstring{$d$}{d} (размерности задачи),
    в зависимости от \texorpdfstring{$\kappa$}{k} (числа обусловленности матрицы \texorpdfstring{$A$}{a}).}

\begin{theorem}
    Метод сопряженных градиентов для решения системы линейных уравнений с
    квадратной положительно определенной матрицей размера $d$ находит точное решение за не более
    чем $d$ итераций
\end{theorem}

\begin{theorem}
    Метод сопряженных градиентов для решения системы линейных уравнений с
    квадратной положительно определенной матрицей размера $d$ находит точное решение за не более
    чем $r$ итераций, где $r$ --– число уникальных собственных значений
    матрицы.
\end{theorem}

\begin{theorem}
    Метод сопряженных градиентов для решения системы линейных уравнений с квадратной положительно определенной матрицей размера $d$ имеет следующую оценку сходимости:
    $$ \|x_k - x^*\|_A \leq 2 \left( \frac{\sqrt{\kappa(A)} - 1}{\sqrt{\kappa(A)} + 1} \right)^k \|x_0 - x^*\|_A, $$
    где $ \|x\|_A = x^T A x $ и $ \kappa(A) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)} $.
\end{theorem}

Характер сходимости --- линейный.
