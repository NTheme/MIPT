\section{Билет 13}

\subsection{Удос. Различные постановки задачи стохастической оптимизации. Итерация метода SGD. Интуиция метода.
    Характер сходимости в условиях ограниченной дисперсии стохастического градиента. Итерация метода SAGA.
    Интуиция метода. Характер сходимости.}

Теперь пусть целевая функция – это математическое ожидание от случайной величины $\xi$ по распределению $D$:
$$\min_{x \in \mathbb{R}^d} \left[ f(x) := \mathbb{E}_{\xi \sim D}[f(x, \xi)] \right]$$

\begin{agreement}
    Данные поступают несмещённо, т.е.
    $$\mathbb{E}_{\xi \sim D}[\nabla f(x, \xi)] = \nabla f(x)$$
\end{agreement}

\begin{definition}
    Часто в машинном обучении начинаем не с нуля, и дана обучающая выборка, тогда задачу обучения записывают в виде минимизации эмпирического риска:
    $$\min_{x \in \mathbb{R}^d} \left[ f(x) := \frac{1}{n} \sum_{i=1}^{n} l(g(x, \xi_{x,i}), \xi_{y,i}) \right],$$

    где $\{\xi_i\}_{i=1}^n$ – выборка из $D$, $g$ –-- модель, $l$ – функция.
    Такую постановку называют оффлайн (данные фиксированы, а не поступают в режиме реального времени).
\end{definition}

\subsection*{SGD}

\begin{algorithm}[H]
    \caption{Алгоритм 45. Стохастический градиентный спуск (SGD)}
    \textbf{Вход:} размеры шагов $\{\gamma_k\}_{k=0}^{K-1} > 0$, стартовая точка $x_0 \in \mathbb{R}^d$, количество итераций $K$
    \begin{algorithmic}[1]
        \For{$k = 0, 1, \dots, K - 1$}
        \State Сгенерировать независимо $\xi_k$
        \State Вычислить стохастический градиент $\nabla f(x_k, \xi_k)$
        \State $x_{k+1} = x_k - \gamma_k \nabla f(x_k, \xi_k)$
        \EndFor
        \State \textbf{Выход:} $x_K$
    \end{algorithmic}
\end{algorithm}

\subsection*{Сумерки. SAGA}

Идея: если считали когда-то градиент для $f_i$, то сохраним его.
$\frac{1}{n} \sum_{j=1}^n y_{k,j}$ – «запаздывающая» версия $\nabla f(x_k)$ (точка не текущая).
$\mathbb{E}[g_k | x_k] = \nabla f(x_k)$

При $x_k \to x^*$ имеем, что $y_{k,j} \to \nabla f_j(x^*)$, и $\frac{1}{n} \sum_{j=1}^n y_{k,j} \to \nabla f(x^*) = 0$.

$\nabla f_{i_k}(x_k) \to \nabla f_j(x^*)$, значит $g_k \to 0$ $\Rightarrow$ эффект редукции дисперсии.

Из минусов: лишняя $O(nd)$ память.

\begin{algorithm}[H]
    \caption{Алгоритм 46. SAGA}
    \textbf{Вход:} размер шага $\gamma > 0$, стартовая точка $x_0 \in \mathbb{R}^d$, значения памяти $y_0^i = 0$ для всех $i \in [n]$, количество итераций $K$
    \begin{algorithmic}[1]
        \For{$k = 0, 1, \dots, K - 1$}
        \State Сгенерировать независимо $i_k$
        \State Вычислить $g_k = \nabla f_{i_k}(x_k) - y_{k, i_k} + \frac{1}{n} \sum_{j=1}^n y_{k, j}$
        \State Обновить $y_{k+1}^i = \left\{
            \begin{array}{ll}
                \nabla f_i(x_k), & \text{если } i = i_k, \\
                y_{k}^i,         & \text{иначе}
            \end{array}
            \right.$
        \State $x_{k+1} = x_k - \gamma g_k$
        \EndFor
        \State \textbf{Выход:} $x_K$
    \end{algorithmic}
\end{algorithm}

\subsection{Хор. Оценки сходимости SGD для сильно выпуклых гладких задач в условиях ограниченной дисперсии
    стохастического градиента. Оценки сходимости SAGA для сильно выпуклых гладких задач вида конечной суммы.}

Дисперсия стохастического градиента ограничена:
$$\mathbb{E}_{\xi} \left[ \left\| \nabla f(x, \xi) - \nabla f(x) \right\|_2^2 \right] \leq \sigma^2$$

Стохастический градиент несмещённый:
$$\mathbb{E}_{\xi}[\nabla f(x, \xi)] = \nabla f(x)$$

\begin{theorem}
    Пусть задача безусловной стохастической оптимизации с $L$-гладкой,
    $\mu$-сильно выпуклой целевой функцией $f$ решается с помощью SGD с $\gamma_k \leq \frac{1}{L}$
    в условиях несмещенности и ограниченности дисперсии стохастического градиента.
    Тогда справедлива следующая оценка сходимости:
    $$\mathbb{E} \left[ \| x_{k+1} - x^* \|_2^2 \right] \leq (1 - \gamma_k \mu) \mathbb{E} \left[ \| x_k - x^* \|_2^2 \right] + \gamma_k^2 \sigma^2$$
\end{theorem}

\begin{theorem}
    Пусть задача безусловной стохастической оптимизации вида конечной суммы с $L$-гладкими, 
    выпуклыми функциями $f_i$ и $\mu$-сильно выпуклой целевой функцией $f$ решается 
    с помощью SAGA с $\gamma \leq \frac{1}{6L}$. 
    Тогда справедлива следующая оценка сходимости:
    $$\mathbb{E} [V_k] \leq \max \left\{ (1 - \mu \gamma), \left(1 - \frac{1}{2n}\right) \right\}^k \mathbb{E} [V_0],$$

    где
    $$V_k = \| x_k - x^* \|_2^2 + 4n \gamma^2 \cdot \frac{1}{n} \sum_{i=1}^n \| y_k^i - \nabla f_i(x^*) \|_2^2.$$

    Получается следующая итерационная сложность:
    $$O \left( \left[ n + \frac{L}{\mu} \right] \log \frac{1}{\epsilon} \right)$$
\end{theorem}
