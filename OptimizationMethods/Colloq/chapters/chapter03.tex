\section{Билет 3}

\subsection{Итерация метода градиентного спуска. Интуиция: почему такой метод,
    зачем нужен параметр (шаг). Характер сходимости (линейная / сублинейная
    / . . . / локальная / глобальная) градиентного спуска для гладких
    сильно выпуклых задач}

Рассмотрим задачу оптимизации $$\min_{x \in \mathbb{R}^d} f(x),$$ где функция $f(x)$ дифференцируема.

\begin{algorithm}[ht]
    \caption{Градиентный спуск}
    \textbf{Вход:} размер шагов $\{\gamma_k\}_{k=0}^{\infty} > 0$, стартовая точка $x_0 \in \mathbb{R}^d$, количество итераций $K$.
    \begin{algorithmic}[1]
        \For{$k = 0, 1, \dots, K - 1$}
        \State Вычислить $\nabla f(x_k)$
        \State $x_{k+1} = x_k - \gamma_k \nabla f(x_k)$
        \EndFor
    \end{algorithmic}
    \textbf{Выход:} $x_K$
\end{algorithm}

Характер сходимости --- линейная.

\subsection{Хор. Формулировка оценки сходимости градиентного спуска для гладких сильно выпуклых задач.}

\begin{theorem}
    Пусть задача безусловной оптимизации (32) с $L$-гладкой, $\mu$-сильно выпуклой целевой функцией $f$ решается 
    с помощью градиентного спуска. Тогда при $\gamma_k = \frac{1}{L}$ справедлива следующая оценка сходимости:
    $$\|x_K - x^*\|_2^2 \leq \left( 1 - \frac{\mu}{L} \right)^K \|x_0 - x^*\|_2^2.$$
    Более того, чтобы добиться точности $\varepsilon$ по аргументу ($\|x_k - x^*\|_2 \leq \varepsilon$), необходимо
    $$K = O\left( \frac{L}{\mu} \log \frac{\|x_0 - x^*\|_2}{\varepsilon} \right) = \tilde{O}\left( \frac{L}{\mu} \right)$$
    итераций.

\end{theorem}

\begin{note}
    Мы будем использовать $O$-нотацию, чтобы «убирать» численные факторы, и $\tilde{O}$-нотацию, 
    чтобы \guillemetleft убирать\guillemetright \, еще и логарифмические факторы. Более формально, все факторы вида 
    $\log(\text{poly}(L, \mu, \varepsilon, \dots))$, где $\text{poly}(L, \mu, \varepsilon, \dots)$ --- полиномы.

\end{note}