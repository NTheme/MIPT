\section{Билет 6}

\subsection{Удав. Итерация метода Ньютона. Интуиция метода Ньютона: почему берем
    именно такую итерацию. Характер сходимости для сильно выпуклых
    задач с Липшицевым гессианом. Квазиньютоновское уравнение, интуиция}

\subsection*{Метод Ньютона}

Рассмотрим задачу поиска \guillemetleft корня\guillemetright \, функции:
Найти такое $t^*$, что $\varphi(t^*) = 0$,
где $\varphi : \mathbb{R} \to \mathbb{R}$.
Пусть мы находимся в точке $t_0$ и хотим найти такую поправку $\Delta t$, что $t_0 + \Delta t \approx t^*$. Разложим $\varphi$ по формуле Тейлора до первого порядка:
$$ \varphi(t_0 + \Delta t) = \varphi(t_0) + \varphi'(t_0) \Delta t + o(\Delta t). $$

Поскольку $t_0 + \Delta t \approx t^*$, то:
$$ \varphi(t_0 + \Delta t) \approx \varphi(t^*) = 0 \quad \Rightarrow \quad \varphi(t_0) + \varphi'(t_0) \Delta t \approx 0. $$

Таким образом,
$$ \Delta t \approx - \frac{\varphi(t_0)}{\varphi'(t_0)}. $$

Продолжая строить новые точки, получаем выражение для итерации метода, предложенного Ньютоном в 17-м веке:
$$ t_{k+1} = t_k - \frac{\varphi(t_k)}{\varphi'(t_k)}. $$

Важно отметить ключевую особенность предложенного метода – сходимость к решению только в некоторой его окрестности.
В качестве примера можно рассмотреть $\varphi(x) = \frac{x}{\sqrt{1+x^2}}$.
Выберем в качестве начального приближения некоторую точку $t_0$. Тогда нетрудно видеть, что:
\begin{itemize}
    \item $|t_0| < 1$ — метод сходится,
    \item $|t_0| = 1$ — метод колеблется в точках -1 и 1,
    \item $|t_0| > 1$ — метод расходится.
\end{itemize}

Интуитивно понятно, что мы ожидаем получить шаг:
$$ x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k). $$
Градиентный спуск работает с линейной аппроксимацией функции.

Теперь будем минимизировать ее квадратичную аппроксимацию:
$$ f(x) \approx f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{1}{2} \langle x - x_k, \nabla^2 f(x_k) (x - x_k) \rangle, $$
где
$$ \nabla f(x_k) + \nabla^2 f(x_k)(x - x_k) = 0 \quad \Rightarrow \quad x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k). $$

\begin{algorithm}[H]
    \caption{Метод Ньютона}
    \textbf{Вход:} стартовая точка $x_0 \in \mathbb{R}^d$, количество итераций $K$
    \begin{algorithmic}[1]
        \For{$k = 0, 1, \dots, K - 1$}
        \State Вычислить $\nabla f(x_k)$, $\nabla^2 f(x_k)$
        \State $x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k)$
        \EndFor
    \end{algorithmic}
    \textbf{Выход:} $x_K$
\end{algorithm}

Характер сходимости --- квадратичный.

\subsection*{Квазиньютоновские методы}

Идея – получить какие-то свойства, присущие гессиану, и вместо $(\nabla^2 f(x))^{-1}$ использовать симметричную матрицу $H_k$,
обладающую теми же свойствами, но при этом более дешевую с точки зрения вычислений.
$$ \nabla f(x_k) - \nabla f(x_{k+1}) \approx \nabla^2 f(x_{k+1})(x_k - x_{k+1}). $$

Откуда:
$$ x_{k+1} - x_k \approx (\nabla^2 f(x_{k+1}))^{-1} (\nabla f(x_{k+1}) - \nabla f(x_k)). $$

Заменим $(\nabla^2 f(x_{k+1}))^{-1}$ на $H_{k+1}$, введем обозначения:
$$ s_k = x_{k+1} - x_k \quad \text{и} \quad y_k = \nabla f(x_{k+1}) - \nabla f(x_k): $$

$$ s_k = H_{k+1} y_k. $$

Получили \textbf{квазиньютоновское уравнение}.

\subsection{Xor, And, Or, Not. Формулировка оценки сходимости метода Ньютона для сильно выпуклых задач с Липшицевым гессианом.
    Способы получения глобальной сходимости для метода Ньютона. Правила обновления матриц H или B для SR1 и BFGS.}

\begin{theorem}
    Пусть задача безусловной минимизации решается методом Ньютона.
    Тогда при обозначенных выше предположениях на целевую функцию справедлива следующая оценка сходимости за одну итерацию:
    $$ \| x_{k+1} - x^* \|_2 \leq \frac{M}{2\mu} \| x_k - x^* \|_2^2. $$
\end{theorem}

\begin{note}
    Сходимость, как и в случае одномерного метода Ньютона, является локальной. А именно, чтобы гарантировать
    $$ \|x_1 - x^*\|_2 < \|x_0 - x^*\|_2, $$
    нужно предположить, что:
    $$ \|x_0 - x^*\|_2 < \frac{2\mu}{M}. $$
\end{note}

\begin{note}
    Существует несколько способов сделать сходимость метода Ньютона глобальной. Один из них — так называемый
    демпфированный метод с шагом
    $$ x_{k+1} = x_k - \gamma_k \left( \nabla^2 f(x_k) \right)^{-1} \nabla f(x_k). $$
    Стоит отметить, что на практике демпфированные методы сходятся
    медленнее оригинального.
\end{note}

\subsection*{Квазиньютоновские методы: SR1}

Будем использовать для обновления матрицы дешевую с точки зрения вычислений добавку:
$$ H_{k+1} = H_k + \mu_k q_k (q_k)^T, $$
где $\mu_k \in \mathbb{R}$ и $q_k \in \mathbb{R}^d$ нужно подобрать.

Подбираем исходя из квазиньютоновского уравнения:
$$ s_k = H_{k+1} y_k = H_k y_k + \mu_k q_k (q_k)^T y_k = H_k y_k + \mu_k \left( (q_k)^T y_k \right) q_k. $$

Откуда
$$ \mu_k \left( (q_k)^T y_k \right) q_k = s_k - H_k y_k. $$

Заметим, что вектор $q_k$ коллинеарен $s_k - H_k y_k$, и выберем
$$ q_k = s_k - H_k y_k. $$

Тогда
$$ \mu_k = \frac{1}{(q_k)^T y_k}. $$

Итак, метод SR1 (Broyden) для подсчета матриц $H$:
$$ H_{k+1} = H_k + \frac{(s_k - H_k y_k)(s_k - H_k y_k)^T}{(s_k - H_k y_k)^T y_k}. $$

\subsection*{Квазиньютоновские методы: BFGS}

Рассмотрим задачу поиска $H_{k+1}$ как задачу поиска «близкой» к $H_k$ матрицы с точки зрения оптимизации:
\begin{equation}
    \begin{aligned}
        & H_{k+1} = \arg \min_{H \in \mathbb{R}^{d \times d}} \| H - H_k \|_2 \\
        & \text{s.t. } s_k = H y_k, \\
        & \phantom{\text{s.t. }} H^T = H.
    \end{aligned}
\end{equation}

В зависимости от выбранной нормы будут получаться разные квазиньютоновские методы. В случае взвешенной нормы Фробениуса $\|A\|_W = \|W^{1/2} A W^{1/2}\|_F$, где $W y_k = s_k$, получим следующее:
$$ H_{k+1} = (I - \rho_k s_k (y_k)^T) H_k (I - \rho_k y_k (s_k)^T) + \rho_k s_k (s_k)^T, $$
где
$$ \rho_k = \frac{1}{(y_k)^T s_k}. $$

Этот метод называется BFGS. Чтобы прийти к этой формуле другим способом, рассмотрим $B_{k+1} = H_{k+1}^{-1}$ и запишем квазиньютоновское уравнение для $B$:
$$ B_{k+1} s_k = y_k. $$

Для $B_{k+1}$ можно написать SR1 пересчет матрицы:
$$ B_{k+1} = B_k + \frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}. $$

Делая двухранговую прибавку из выражения для $B_{k+1}$ и подгоняя параметры, получаем:
$$ B_{k+1} = B_k + \frac{y_k (y_k)^T}{(y_k)^T s_k} + B_k s_k (B_k s_k)^T \frac{(s_k)^T B_k s_k}{(s_k)^T B_k s_k}. $$

Остается только обратить $B_{k+1}$ (по формуле Шермана-Маррисона-Вудберри).
